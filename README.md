# ğŸ¤ Human Emotion Detection from Voice

This project detects human emotions using voice input. It uses audio processing and machine learning techniques to classify emotions such as happy, sad, angry, calm, and more.

## ğŸ“Œ Objective

To build a machine learning model that detects emotions from speech and provides a simple Streamlit web interface for users to interact and test.

## ğŸ“š Dataset

- **RAVDESS (Ryerson Audio-Visual Database of Emotional Speech and Song)**  
  The dataset contains 1440 `.wav` audio files representing 8 different emotions.

## ğŸ› ï¸ Tools & Libraries Used

- Python
- NumPy, Pandas, Matplotlib, Seaborn
- Librosa (Audio processing)
- Scikit-learn (Model training)
- Joblib (Model serialization)
- Streamlit (Web interface)

## ğŸš€ Steps Involved

1. **Data Preparation**
   - Copy `.wav` files from RAVDESS into `emotions_dataset/`
   - Extract labels from filenames

2. **Feature Extraction**
   - Use Librosa to extract MFCC, Chroma, and Mel Spectrogram features from audio

3. **Model Training**
   - Train a Random Forest Classifier on extracted features
   - Evaluate with confusion matrix and classification report

4. **Model Saving**
   - Save trained model using `joblib` in `model/` folder

5. **Streamlit Web App**
   - Build UI to upload audio file
   - Predict and display emotion
   - Show optional emotion trend graph
   - Save prediction history/session data
